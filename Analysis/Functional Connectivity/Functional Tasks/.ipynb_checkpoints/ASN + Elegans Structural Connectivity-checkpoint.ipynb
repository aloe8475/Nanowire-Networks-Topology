{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/suphys/aloe8475\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/import/silo2/aloe8475/Documents/edamame\n"
     ]
    }
   ],
   "source": [
    "cd \"Documents/edamame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from edamame import *\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import edamame.core.wires as wires\n",
    "from random import choice\n",
    "import warnings\n",
    "from IPython.core.debugger import set_trace\n",
    "import nct\n",
    "import bct\n",
    "\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "def compressed_pickle(obj, filename,protocol=-1):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        cPickle.dump(obj, f, protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_pickle(file):\n",
    "    with gzip.open(file, 'rb') as f:\n",
    "        loaded_object = cPickle.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_seed2=700\n",
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Largest Components\n",
    "def select_largest_component_new(wires_dict):\n",
    "    \"\"\"\n",
    "    Find and select largest connected component of the original graph G.\n",
    "    Throws away unconnected components and updates all the keys in wires_dict \n",
    "    \"\"\"\n",
    "#     def connected_component_subgraphs(G):\n",
    "#         for c in nx.connected_components(G):\n",
    "#             yield G.subgraph(c)\n",
    "    \n",
    "    wires_dict['G'] = max(connected_component_subgraphs(wires_dict['G']), key=len)\n",
    "#     set_trace()\n",
    "    nw = len(wires_dict['G'].nodes())\n",
    "    nj = len(wires_dict['G'].edges())   \n",
    "    \n",
    "    logging.info(\"The largest component has %5d nodes and %6d edges\", nw, nj)\n",
    "\n",
    "    # Replace values in the dictionary\n",
    "    wires_dict['number_of_wires']     = nw\n",
    "    wires_dict['number_of_junctions'] = nj\n",
    "    wires_dict['xa'] = wires_dict['xa'][wires_dict['G'].nodes()] \n",
    "    wires_dict['ya'] = wires_dict['ya'][wires_dict['G'].nodes()] \n",
    "    wires_dict['xb'] = wires_dict['xb'][wires_dict['G'].nodes()] \n",
    "    wires_dict['yb'] = wires_dict['yb'][wires_dict['G'].nodes()]\n",
    "    wires_dict['xc'] = wires_dict['xc'][wires_dict['G'].nodes()] \n",
    "    wires_dict['yc'] = wires_dict['yc'][wires_dict['G'].nodes()]\n",
    " \n",
    "    # Keep old edge_list\n",
    "    old_edge_list = [(ii, kk) for ii, kk in  zip(wires_dict['edge_list'][:, 0], wires_dict['edge_list'][:, 1])]\n",
    "    # Remove old edge list\n",
    "    wires_dict = wires.remove_key(wires_dict, 'edge_list') \n",
    "    # Save indices of intersections in the old graph\n",
    "    ind_dict = {key:value for value,key in enumerate(old_edge_list)}\n",
    "    new_edge_list = sorted([kk if kk[0] < kk[1] else (kk[1], kk[0]) for kk in wires_dict['G'].edges()], key=lambda x: x[0])\n",
    "    # Find intersection between the two sets\n",
    "    inter = set(ind_dict).intersection(new_edge_list)\n",
    "    # Retrieve edge indices/positions from the old list\n",
    "    edges_idx = [ind_dict[idx] for idx in inter]\n",
    "       \n",
    "    # These have length equal to number of junctions -- only get the ones we need\n",
    "    wires_dict['xi'] = wires_dict['xi'][edges_idx] \n",
    "    wires_dict['yi'] = wires_dict['yi'][edges_idx] \n",
    "    \n",
    "    # Get contiguous numbering of nodes\n",
    "    # Build node mapping \n",
    "    node_mapping    = {key:value for value, key in enumerate(sorted(wires_dict['G'].nodes()))}\n",
    "    # This  step also renames edges list\n",
    "    wires_dict['G'] =  nx.relabel_nodes(wires_dict['G'] , node_mapping)\n",
    "\n",
    "    # Swap node vertices if vertex 0 is larger than vertex 1, then sort by first element\n",
    "    wires_dict['edge_list'] = np.asarray(sorted([kk if kk[0] < kk[1] else (kk[1], kk[0]) for kk in wires_dict['G'].edges()], key=lambda x: x[0]))\n",
    "    \n",
    "    # Save adjacency matrix of new graph\n",
    "    wires_dict = wires.remove_key(wires_dict, 'adj_matrix') \n",
    "    wires_dict = wires.generate_adj_matrix(wires_dict)\n",
    "\n",
    "    wire_distances = wires.cdist(np.array([wires_dict['xc'], wires_dict['yc']]).T, np.array([wires_dict['xc'], wires_dict['yc']]).T, metric='euclidean')    \n",
    "    wires_dict['wire_distances'] = wire_distances\n",
    "\n",
    "    return wires_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=1.)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 300nw ASN + C. Elegans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cElegans=loadmat(\"../CODE/Data/Organic Networks Connectomes/celegans277neurons.mat\")\n",
    "\n",
    "    #C:\\Users\\aloe8475\\Documents\\PhD\\GitHub\\CODE\\Data\\Organic Networks Connectomes\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up dictionary for celegans:\n",
    "Elegans={'adj_matrix':[],'G':[],'Accuracy':{'Linear Transformation':[],'Memory Capacity':[]},'Graph Theory':{'Small World':[],'Modularity':[],'CCoeff':[],'MZ':[],'PCoeff':[],'PL':[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elegansMat=cElegans['celegans277matrix']\n",
    "elegansGraph = nx.from_numpy_array(elegansMat)\n",
    "Elegans['adj_matrix']=elegansMat\n",
    "Elegans['G']=elegansGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/import/silo2/aloe8475/Documents/CODE/Analysis/Functional Connectivity/Functional Tasks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile('networks_LinearTransformation.pkl')): #if we haven't saved the file\n",
    "    print('Creating Parameters')\n",
    "    #Set Paramaters for network generation: Centroid Dispersion, Wire Dispersion + Length of Wires\n",
    "    params={'Centroid':np.arange(150,400,25),'Wire Dispersion':[1.0, 2.5, 5.0, 10.0, 25.0, 50.0, 75.0, 80.0, 90.0, 100.0],'Length':np.arange(100,350,25)}\n",
    "    numNetworks=len(params['Centroid'])+len(params['Wire Dispersion'])+len(params['Length']) #all parameters)\n",
    "    if 'ASN300' in locals():\n",
    "        loaded=True\n",
    "    else:\n",
    "        loaded=False\n",
    "        ASN300=[[None]*10 for i in range(numNetworks)]\n",
    "#         cluster1=[[None]*10 for i in range(len(params['Centroid']))] #change in Centroid Dispersion\n",
    "#         cluster2=[[None]*10 for i in range(len(params['Wire Dispersion']))] #change in Wire Dispersion\n",
    "#         cluster3=[[None]*10 for i in range(len(params['Length']))] #change in average wire length\n",
    "\n",
    "\n",
    "    #loop through these:\n",
    "    centroid2=params['Centroid']\n",
    "    length2=params['Length']\n",
    "    disp2=params['Wire Dispersion']\n",
    "    for i in range(49):\n",
    "        temp=params['Centroid']\n",
    "        temp2=params['Length']\n",
    "        temp3=params['Wire Dispersion']\n",
    "        centroid2=np.concatenate((centroid2,temp))\n",
    "        length2=np.concatenate((length2,temp2))\n",
    "        disp2=np.concatenate((disp2,temp3))\n",
    "    print('Parameters Saved')\n",
    "else:\n",
    "    print('Parameters Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17/06/2020 - more realisations of each parameter in the sweep:\n",
    "\n",
    "We should generate 3 - 10 networks of each of the 300, and then take the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loops through 30 different parameters, and for each parameter generates networks until we find 10 networks with 277+ nodes,\n",
    "# for a total of 300 networks of 277+ nodes with different parameters.\n",
    "\n",
    "\n",
    "#I create 30 sets of networks. For each set, i generate networks, changing 2 parameters, \n",
    "# until i've found 10 networks with >277 nodes. \n",
    "# For each parameter, if we have <277, we loop through 10 more networks with different random seeds (same parameter), \n",
    "# to see if we can get >277 nodes, then we move on to next parameter\n",
    "\n",
    "#This way I generate 300 networks with varying parameters.\n",
    "\n",
    "if (not os.path.isfile('networks_LinearTransformation.pkl')): #if we haven't saved the file\n",
    "    print('Networks not loaded, creating now')\n",
    "    count1=0\n",
    "    count2=0\n",
    "    count3=0\n",
    "    this_seed=1779#np.random.randint(10000)\n",
    "#     seed2=range(1, 6001, 20)\n",
    "    for i in range(numNetworks):\n",
    "        #loop through different parameter sets:\n",
    "        x=0\n",
    "        for j in range(500): #for each parameter, create 100 different networks\n",
    "            if i < len(params['Centroid']):\n",
    "#                 seed2=np.random.randint(100000)\n",
    "                #Change centroid dispersion + avg wire length, but keep dispersion constant\n",
    "                if x < 10:\n",
    "                    print('Parameter: ' + str(i+1) + ' , Network: ' + str(j+1))\n",
    "                    print('Parameter 1: Centroid ' +str(params['Centroid'][i])+ ' , Parameter 2: Avg Length ' + str(length2[j]))\n",
    "                    temp=wires.generate_wires_distribution(300,this_seed=np.random.randint(100000),wire_dispersion=10,centroid_dispersion=params['Centroid'][i],wire_av_length=length2[j])\n",
    "                    temp=wires.detect_junctions(temp)\n",
    "                    temp=wires.generate_adj_matrix(temp)\n",
    "                    temp=wires.generate_graph(temp)\n",
    "                    temp=select_largest_component_new(temp)\n",
    "                    counter1=0\n",
    "                    \n",
    "                while temp['G'].number_of_nodes() <277 and counter1<=10: #if nodes are less than 277, try the same parameters again for 10 more times before moving on to the next parameter\n",
    "                    print(str(counter1) +': Parameter 1: Centroid ' +str(params['Centroid'][i])+ ' , Parameter 2: Avg Length ' + str(length2[j]))\n",
    "                    temp=wires.generate_wires_distribution(300,this_seed=np.random.randint(100000),wire_dispersion=10,centroid_dispersion=params['Centroid'][i],wire_av_length=length2[j])\n",
    "                    temp=wires.detect_junctions(temp)\n",
    "                    temp=wires.generate_adj_matrix(temp)\n",
    "                    temp=wires.generate_graph(temp)\n",
    "                    temp=select_largest_component_new(temp)\n",
    "                    counter1=counter1+1\n",
    "                    \n",
    "                if temp['G'].number_of_nodes()>=277: #only networks with more than 277 nodes\n",
    "                    if x < 10: #only store 10 networks of each type\n",
    "                        ASN300[i][x]=temp         \n",
    "                        cluster1[count1][x]=temp    \n",
    "                    x = x+1\n",
    "                else:\n",
    "                    print('saved networks ' + str(x))\n",
    "\n",
    "                #only increase count if it's the last j value in the loop\n",
    "                if j == 499:\n",
    "                    count1=count1+1\n",
    "\n",
    "            elif i >= len(params['Centroid']) and i < len(params['Centroid'])+len(params['Wire Dispersion']):\n",
    "                #Change centroid dispersion + wire dispersion, keep avg wire length constant\n",
    "                if x < 10:\n",
    "                    print('Parameter: ' + str(i+1) + ' , Network: ' + str(j+1))\n",
    "                    print('Parameter 1: Wire Disp ' +str(params['Wire Dispersion'][i-len(params['Centroid'])])+ ' , Parameter 2: Centroid ' + str(centroid2[j]))\n",
    "                    temp=wires.generate_wires_distribution(300,this_seed=np.random.randint(100000),centroid_dispersion=centroid2[j],wire_dispersion=params['Wire Dispersion'][i-len(params['Centroid'])],wire_av_length=110)\n",
    "                    temp=wires.detect_junctions(temp)\n",
    "                    temp=wires.generate_adj_matrix(temp)\n",
    "                    temp=wires.generate_graph(temp)\n",
    "                    temp=select_largest_component_new(temp)\n",
    "                    \n",
    "                    counter2=0\n",
    "                while temp['G'].number_of_nodes() <277 and counter2<=10:\n",
    "                    print(str(counter2) +': Parameter 1: Wire Disp ' +str(params['Wire Dispersion'][i-len(params['Centroid'])])+ ' , Parameter 2: Centroid ' + str(centroid2[j]))\n",
    "                    temp=wires.generate_wires_distribution(300,this_seed=np.random.randint(100000),centroid_dispersion=centroid2[j],wire_dispersion=params['Wire Dispersion'][i-len(params['Centroid'])],wire_av_length=110)\n",
    "                    temp=wires.detect_junctions(temp)\n",
    "                    temp=wires.generate_adj_matrix(temp)\n",
    "                    temp=wires.generate_graph(temp)\n",
    "                    temp=select_largest_component_new(temp)\n",
    "                    counter2=counter2+1\n",
    "                    \n",
    "                if temp['G'].number_of_nodes()>=277: #only networks with more than 250 nodes\n",
    "                    if x < 10: #only store 10 networks of each type\n",
    "                        ASN300[i][x]=temp         \n",
    "                        cluster2[count2][x]=temp\n",
    "                    x = x+1\n",
    "                else:\n",
    "                    print('saved networks ' + str(x))\n",
    "\n",
    "                #only increase count if it's the last j value in the loop\n",
    "                if j == 499:\n",
    "                    count2=count2+1        \n",
    "            else:\n",
    "               #Change  wire dispersion +  avg wire length, keep centroid dispersion constant\n",
    "                if x < 10:\n",
    "                    print('Parameter: ' + str(i+1) + ' , Network: ' + str(j+1))\n",
    "                    print('Parameter 1 Avg Length:  ' +str(params['Length'][i-(len(params['Centroid'])+len(params['Wire Dispersion']))])+ ' , Parameter 2: Wire Disp ' + str(disp2[j]))\n",
    "                    temp=wires.generate_wires_distribution(300,this_seed=np.random.randint(100000),centroid_dispersion=300,wire_dispersion=disp2[j],wire_av_length=params['Length'][i-(len(params['Centroid'])+len(params['Wire Dispersion']))])\n",
    "                    temp=wires.detect_junctions(temp)\n",
    "                    temp=wires.generate_adj_matrix(temp)\n",
    "                    temp=wires.generate_graph(temp)\n",
    "                    temp=select_largest_component_new(temp)\n",
    "                    \n",
    "                    counter3=0\n",
    "                while temp['G'].number_of_nodes() <277  and counter3<=10:\n",
    "                    print(str(counter3) +': Parameter 1 Avg Length:  ' +str(params['Length'][i-(len(params['Centroid'])+len(params['Wire Dispersion']))])+ ' , Parameter 2: Wire Disp ' + str(disp2[j]))\n",
    "                    temp=wires.generate_wires_distribution(300,this_seed=np.random.randint(100000),centroid_dispersion=300,wire_dispersion=disp2[j],wire_av_length=params['Length'][i-(len(params['Centroid'])+len(params['Wire Dispersion']))])\n",
    "                    temp=wires.detect_junctions(temp)\n",
    "                    temp=wires.generate_adj_matrix(temp)\n",
    "                    temp=wires.generate_graph(temp)\n",
    "                    temp=select_largest_component_new(temp)\n",
    "                    counter3=counter3+1\n",
    "                    \n",
    "                if temp['G'].number_of_nodes()>=277: #only networks with more than 250 nodes\n",
    "                    if x < 10: #only store 10 networks of each type\n",
    "                        ASN300[i][x]=temp   \n",
    "                        cluster3[count3][x]=temp\n",
    "                    x = x+1\n",
    "                else:\n",
    "                    print('saved networks ' + str(x))\n",
    "    #             ASN300[i][j]=wires.generate_wires_distribution(300,this_seed=seed2[j],centroid_dispersion=300,wire_dispersion=params['Wire Dispersion'][j],wire_av_length=params['Length'][i-(len(params['Centroid'])+len(params['Wire Dispersion']))],)\n",
    "    #             count = 0;\n",
    "    #             cluster3[count3][j]=wires.generate_wires_distribution(300,this_seed=seed2[j],centroid_dispersion=300,wire_dispersion=params['Wire Dispersion'][j],wire_av_length=params['Length'][i-(len(params['Centroid'])+len(params['Wire Dispersion']))],)\n",
    "\n",
    "                #only increase count if it's the last j value in the loop\n",
    "                if j == 499:\n",
    "                    count3=count3+1\n",
    "                    \n",
    "    #Save networks so we don't have to run this every time\n",
    "    name='networks_LinearTransformation.pkl'\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump([ASN300,cluster1,cluster2,cluster3], f)\n",
    "        \n",
    "else: #load pickle file + communicability matrix calculated in Linear Transformation section \n",
    "    name='networks_LinearTransformation.pkl'\n",
    "    print('Loading Networks + Nonlinear Transformation Results')\n",
    "    file = open(name, 'rb')\n",
    "#     [ASN300,cluster1,cluster2,cluster3,time_index,nodesList] = pickle.load(file)\n",
    "#     [ASN300,cluster1,cluster2,cluster3] = pickle.load(file)\n",
    "    [ASN300] = pickle.load(file)\n",
    "\n",
    "    print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up accuracy dictionary for ASN\n",
    "numberOfNodeTests=6\n",
    "if (not os.path.isfile('networks_LinearTransformation.pkl')): #if we haven't saved the file\n",
    "    for i in range(len(ASN300)):\n",
    "        for j in range(len(ASN300[i])):\n",
    "            ASN300[i][j].update({'Accuracy':{'Linear Transformation':[None]*numberOfNodeTests,'Memory Capacity':[]},'Graph Theory':{'Small World':[],'Modularity':[],'CCoeff':[],'MZ':[],'PCoeff':[],'PL':[]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Detect junctions, create adj matrix and graph, and store number of junctions\n",
    "nwEdges300=[]\n",
    "numNodes=[]\n",
    "\n",
    "for i in range(len(ASN300)):\n",
    "    for j in range(len(ASN300[i])):\n",
    "        nwEdges300.append(np.sum(ASN300[i][j]['adj_matrix'])/2)\n",
    "        numNodes.append(ASN300[i][j]['number_of_wires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fwASN100=nx.floyd_warshall_numpy(ASN100['G']) #Find all-pairs shortest path lengths using Floydâ€™s algorithm\n",
    "# fwASN300=[None]*len(ASN300)\n",
    "# for i in range(len(ASN300)):\n",
    "#     fwASN300[i]=nx.floyd_warshall_numpy(ASN300[i]['G'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export adj matrices to calculate small worldness in matlab:\n",
    "# adj_mats={\"AdjMat\":[[None]*len(ASN300[0]) for i in range(len(ASN300))]}\n",
    "# for i in range(len(ASN300)):\n",
    "#     for j in range(len(ASN300[i])):\n",
    "#         adj_mats['AdjMat'][i][j]=(ASN300[i][j]['adj_matrix'])\n",
    "# savemat('300nwASN_multipleNetworks.mat',adj_mats)\n",
    "# #C:\\Users\\aloe8475\\Documents\\PhD\\GitHub\\CODE\\Analysis\\Functional Connectivity\\Functional Tasks\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile('networks_LinearTransformation.pkl')): #if we haven't saved the file\n",
    "    print('Setting up Graph Theory')\n",
    "    for i in range(len(ASN300)):\n",
    "        for j in range(len(ASN300[i])):\n",
    "            ASN300[i][j].update({'Graph Theory':{'Small World':[],'Modularity':[],'CCoeff':[],'MZ':[],'PCoeff':[],'PL':[]}})\n",
    "else:\n",
    "    print('Graph Theory Already Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Worldness: \n",
    "# ------------------------------------\n",
    "# CALCULATED IN MATLAB: smallworldness.m \n",
    "# found in C:\\Users\\aloe8475\\Documents\\PhD\\GitHub\\CODE\\Analysis\\Functional Connectivity\\Functional Tasks\n",
    "# ------------------------------------\n",
    "temp=loadmat(r'300nwASN_multipleNetworks_smallworld.mat')\n",
    "smallworld=temp['smallworld']\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile('networks_LinearTransformation.pkl')): #if we haven't saved the file\n",
    "    # Modularity, PCoeff, Small Worldness & MZ:\n",
    "    ci = []\n",
    "    pcoeff= []\n",
    "    mz= []\n",
    "    clustering = []\n",
    "    count1=0\n",
    "    count2=0\n",
    "    count3=0\n",
    "    for i in tqdm(range(len(ASN300))):\n",
    "        for j in tqdm(range(len(ASN300[i]))):\n",
    "            ci,q=nct.community_louvain(ASN300[i][j]['adj_matrix'])\n",
    "            pcoeff=bct.participation_coef(ASN300[i][j]['adj_matrix'],ci)\n",
    "            mz=bct.module_degree_zscore(ASN300[i][j]['adj_matrix'],ci)\n",
    "            clustering=nx.clustering(ASN300[i][j]['G'])\n",
    "            ASN300[i][j]['Graph Theory']['PL']=dict(nx.all_pairs_shortest_path_length(ASN300[i][j]['G']))\n",
    "            ASN300[i][j]['Graph Theory']['Modularity']=ci\n",
    "            ASN300[i][j]['Graph Theory']['Modularity Score']=q\n",
    "            ASN300[i][j]['Graph Theory']['PCoeff']=pcoeff\n",
    "            ASN300[i][j]['Graph Theory']['MZ']=mz\n",
    "            ASN300[i][j]['Graph Theory']['Small World']=smallworld[i][j]\n",
    "            ASN300[i][j]['Graph Theory']['CCoeff']=clustering\n",
    "            ASN300[i][j]['Graph Theory']['Degree']=nx.degree(ASN300[i][j]['G'])\n",
    "            ASN300[i][j]['Graph Theory']['AvgPL']=nx.average_shortest_path_length(ASN300[i][j]['G'])\n",
    "    #Save networks so we don't have to run this every time\n",
    "    name='networks_LinearTransformation.pkl'\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump([ASN300], f)   \n",
    "else:\n",
    "    print('Graph Theory Already Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More Graph Theory:\n",
    "#Number of Junctions\n",
    "junctions=[]\n",
    "#Number of Nodes:\n",
    "nodes=[]\n",
    "avgPL=[]\n",
    "for i in range(len(ASN300)):\n",
    "    for j in range(len(ASN300[i])):\n",
    "        junctions.append(ASN300[i][j]['number_of_junctions'])\n",
    "        nodes.append(ASN300[i][j]['number_of_wires'])\n",
    "\n",
    "junctions=np.asarray(junctions)\n",
    "nodes=np.asarray(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/import/silo2/aloe8475/Documents/CODE/Data/Functional Connectivity\n"
     ]
    }
   ],
   "source": [
    "cd \"/import/silo2/aloe8475/Documents/CODE/Data/Functional Connectivity/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Export Degree to matlab for BIC analysis\n",
    "\n",
    "# import scipy.stats as stats    \n",
    "# degree=[None]*300\n",
    "# pcoeff=[None]*300\n",
    "# mz=[None]*300\n",
    "# centrality=[None]*300\n",
    "# #Gamma, Exponential, WB, Power Law Fitting:\n",
    "# count=0\n",
    "# for i in range(len(ASN300)):\n",
    "#     for j in range(len(ASN300[i])):\n",
    "#         degree[count]=(list(dict(ASN300[i][j]['Graph Theory']['Degree']).values()))  \n",
    "#         pcoeff[count]=(list(ASN300[i][j]['Graph Theory']['PCoeff']))        \n",
    "#         centrality[count]=(list(nx.betweenness_centrality(ASN300[i][j]['G']).values()))  \n",
    "#         mz[count]=(list(ASN300[i][j]['Graph Theory']['MZ']))\n",
    "#         count+=1\n",
    "    \n",
    "# savemat('Degree.mat',{'pcoeff':pcoeff,'degree':degree,'maxIdx':max_idx_acc,'minIdx':min_idx_acc,'centrality':centrality,'mz':mz})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Elegans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small world calculated on C Elegans Matrix in smallworld.m in MATLAB\n",
    "temp=loadmat(r'cElegans_smallworld.mat')\n",
    "smallworld_elegans=temp['cElegansSW'][0]\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modularity, PCoeff, Small Worldness & MZ:\n",
    "if (not os.path.isfile('elegans_LinearTransformation.pkl')): #if we haven't saved the file\n",
    "    ci = []\n",
    "    pcoeff= []\n",
    "    mz= []\n",
    "\n",
    "    ci,q=nct.community_louvain(elegansMat)\n",
    "    pcoeff=bct.participation_coef(elegansMat,ci)\n",
    "    mz=bct.module_degree_zscore(elegansMat,ci)\n",
    "    Elegans['Graph Theory']['MZ']=mz\n",
    "    Elegans['Graph Theory']['PCoeff']=pcoeff\n",
    "    Elegans['Graph Theory']['Modularity']=ci\n",
    "    Elegans['Graph Theory']['Modularity Score']=q\n",
    "    Elegans['Graph Theory']['Small World']=smallworld_elegans\n",
    "    Elegans['Graph Theory']['PL']=dict(nx.all_pairs_shortest_path_length(elegansGraph))\n",
    "    Elegans['Graph Theory']['CCoeff']=nx.clustering(elegansGraph)\n",
    "    Elegans['Graph Theory']['Degree']=nx.degree(elegansGraph)\n",
    "\n",
    "   #Save networks so we don't have to run this every time\n",
    "    print('Saving Elegans Networks')\n",
    "    name='elegans_LinearTransformation.pkl'\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump([Elegans], f)\n",
    "\n",
    "else: #load pickle file + communicability matrix calculated in Linear Transformation section \n",
    "    name='elegans_LinearTransformation.pkl'\n",
    "    print('Loading Elegans Networks')\n",
    "    file = open(name, 'rb')\n",
    "#     [ASN300,cluster1,cluster2,cluster3,time_index,nodesList] = pickle.load(file)\n",
    "#     [ASN300,cluster1,cluster2,cluster3] = pickle.load(file)\n",
    "    [Elegans] = pickle.load(file)\n",
    "\n",
    "    print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
